{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ 은닉계층 1개로 구성된 다층 신경망 ]\n",
    "### 설명을 위해 입력을 입력층, 출력을 출력층이라 서술하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "def randomize(): np.random.seed(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_MEAN = 0\n",
    "RND_STD = 0.0030\n",
    "\n",
    "LEARNING_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터를 불러오는 함수\n",
    "# csv 파일을 불러오는 함수입니다. 훈련 데이터들을 불러와서 새로운 변수에 담습니다.\n",
    "# 데이터의 형태는 8x6 숫자이미지를 csv 파일화 한것입니다.\n",
    "# 데이터의 형태 즉, 파일의 1행의 형태는 데이터 + 정답으로 이루어 지며, 1 x 52 가 됩니다.\n",
    "# 정답은 이진수 bit 4개로 표현하였습니다.\n",
    "def load_dataset():\n",
    "    with open('./data_set2.csv') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        rows = []\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "            \n",
    "    global data, input_cnt, output_cnt\n",
    "    input_cnt = 48\n",
    "    output_cnt = 4\n",
    "\n",
    "    data = np.zeros([len(rows), input_cnt+output_cnt])\n",
    "    output = np.zeros(4) \n",
    "    \n",
    "    for n, row in enumerate(rows):\n",
    "        data[n] = row\n",
    "\n",
    "# 테스트 데이터를 불러오는 함수        \n",
    "# csv 파일을 불러오는 함수입니다. 테스트 데이터들을 불러와서 새로운 변수에 담습니다.\n",
    "# 데이터의 형태는 8x6 숫자이미지를 csv 파일화 한것입니다.\n",
    "# 데이터의 형태 즉, 파일의 1행의 형태는 데이터 + 정답으로 이루어 지며, 1 x 52 가 됩니다.\n",
    "# 정답은 이진수 bit 4개로 표현하였습니다.\n",
    "def load_testset():\n",
    "    with open('./train_set2.csv') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        rows = []\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "            \n",
    "    global test, input_cnt, output_cnt\n",
    "    input_cnt = 48\n",
    "    output_cnt = 4\n",
    "\n",
    "    test = np.zeros([len(rows), input_cnt+output_cnt])\n",
    "    output = np.zeros(4) \n",
    "    \n",
    "    for n, row in enumerate(rows):\n",
    "        test[n] = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설계한 신경망 모델을 초기화하는 함수입니다. \n",
    "# 신경망에 있어서 입력, 출력, 은닉계층 노드의 수에 대해 설정합니다.\n",
    "# 또한, 은닉 계층 및 출력 계층에 대한 가중치 행렬을 생성합니다.\n",
    "#weight_hidden, weight_output\n",
    "def init_model_node():\n",
    "    global weight_output, weight_hidden, input_cnt, output_cnt, hidden_cnt\n",
    "    \n",
    "    input_cnt = 48\n",
    "    output_cnt = 4\n",
    "    hidden_cnt = 10\n",
    "    \n",
    "    # 가중치 행렬 생성\n",
    "    # input data의 수 48\n",
    "    # 48개의 데이터가 한번에 들어가게 된다.\n",
    "    # output data의 수 4 \n",
    "    # 4개의 이진수 데이터로 구성된다.\n",
    "    # weight_hidden => 은닉층 계층의 가중치 행렬\n",
    "    # weight_output => 출력층 계층의 가중치 행렬\n",
    "    weight_hidden = alloc_param_pair([input_cnt, hidden_cnt])\n",
    "    weight_output = alloc_param_pair([hidden_cnt, output_cnt])\n",
    "    \n",
    "    \n",
    "# 가중치 행렬 생성을 위한 함수입니다.\n",
    "# 난수를 이용해 가중치 행렬이 일정한 값으로 유지되지 않도록 설정해줍니다.\n",
    "def alloc_param_pair(shape):\n",
    "    weight = np.random.normal(RND_MEAN, RND_STD, shape)\n",
    "    bias = np.zeros(shape[-1])\n",
    "    return {'w':weight, 'b':bias}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터를 나누는 함수입니다.\n",
    "# 불러온 파일에 대해서 데이터와 정답으로 나눕니다.\n",
    "# 즉, 8x6 그림 파일에 대한 데이터와 그에 대한 정답 4bit로 나누는 과정입니다.\n",
    "def get_train_data():\n",
    "    global data, output_cnt\n",
    "    train_data = data\n",
    "    return train_data[:, :-output_cnt], train_data[:, -output_cnt:]\n",
    "\n",
    "# 테스트 데이터를 나누는 함수입니다.\n",
    "# 불러온 파일에 대해서 데이터와 정답으로 나눕니다.\n",
    "# 즉, 8x6 그림 파일에 대한 데이터와 그에 대한 정답 4bit로 나누는 과정입니다.\n",
    "def get_test_data():\n",
    "    global test, input_cnt, output_cnt\n",
    "    test_data = test\n",
    "    return test_data[:, :-output_cnt], test_data[:, -output_cnt:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    print('은닉 계층 하나를 갖는 다층 퍼셉트론이 작동되었습니다.')\n",
    "    init_model_node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전방향으로 학습이 진행되도록 하는 함수입니다.\n",
    "# forward_neuralnet_hidden1\n",
    "# \n",
    "def forward_net(x):\n",
    "    return forward_net_hidden(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 싱글 레이어 퍼셉트론에서처럼 전방향으로 학습을 진행하는 함수입니다.\n",
    "# hidden -> 입력층과 은닉계층 사이의 가중치 행렬 과 입력의 곱으로 형성됩니다.\n",
    "# output -> 은닉계층과 출력계층 사이의 가중치 행렬 과 은닉계층의 출력의 곱으로 형성됩니다.\n",
    "def forward_net_hidden(x):\n",
    "    global data, weight_output, weight_hidden\n",
    "    \n",
    "    # forward porpagation 부분입니다.\n",
    "   \n",
    "    # 이 부분을 통해 입력은 은닉층을거쳐 출력층까지 전달됩니다.\n",
    "    # 1 X 48 =>  1개의 INPUT DATA \n",
    "    # 48 X 10 => 1번째 계층의 가중치 행렬\n",
    "    \n",
    "    # hidden --> 10 X 4 =>  2번째 계층의 가중치 행렬 \n",
    "    # output1 -->  X 4 => 최종 결과를 포함하고 있는 행렬\n",
    "    hidden = sigmoid(np.matmul(x, weight_hidden['w']) + weight_hidden['b'])\n",
    "    output = sigmoid(np.matmul(hidden, weight_output['w']) + weight_output['b'])\n",
    "  \n",
    "    return output, hidden\n",
    "\n",
    "# 활성화 함수 - Sigmoid 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 +np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 계층 부분에 대한 델타 값을 구하기 위한 함수입니다.\n",
    "# PDF의 공식대로 델타를 구하도록 합니다.\n",
    "# y X (1-y) X * (T - Y)\n",
    "# T -> y\n",
    "# y -> output\n",
    "def output_delta(output, y):\n",
    "    diff = y-output\n",
    "    diff_y = 1 - output\n",
    "    multiple_diff = diff * diff_y\n",
    "    result = output * multiple_diff\n",
    "    return result\n",
    "\n",
    "# 은닉계층의 델타값에 대해 (Z X (1-Z))를 구하기 위한 부분입니다.\n",
    "def output_hidden(hidden):\n",
    "    return hidden * (1-hidden)\n",
    "\n",
    "\n",
    "# 정확도를 계산하기 위한 \n",
    "def evaluation(output, y):\n",
    "    count_answer = 0\n",
    "    arr_len = len(output)\n",
    "    for index in range(arr_len) :\n",
    "        if(np.array_equiv(output[index] ,y[index])):\n",
    "            count_answer += 1\n",
    "            \n",
    "    final_acc = count_answer / arr_len\n",
    "    return final_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체적인 학습을 진행하는 함수입니다.\n",
    "# forward, backward\n",
    "# 전방향, 역방향 모두 학습을 진행시킵니다.\n",
    "# 인자 x => 학습 데이터     인자 y => 학습 데이터에 대한 정답\n",
    "# 데이터의 수만큼 반복문을 통해 학습을 진행합니다.\n",
    "def run_train(x, y):\n",
    "    # < Forward 부분 >\n",
    "    global data, weight_output, weight_hidden\n",
    "    \n",
    "    # i -> 데이터의 갯수\n",
    "    # x[n] -> n 번째 행의 데이터를 의미합니다.\n",
    "    i = len(data)\n",
    "    for n in range(i):\n",
    "\n",
    "        # output -> forward 계산을 통한 output\n",
    "        # hidden -> 은닉계층의 결과값\n",
    "        # 1 x 4 , 1 x 10\n",
    "        output, hidden = forward_net_hidden(x[n])\n",
    "\n",
    "        # output delta 구하기\n",
    "        # 출력층의 델타값에 대한 행렬은 1 x 4로 구성됩니다. \n",
    "        delta_output = output_delta(output, y[n])\n",
    "\n",
    "        # 출력층으로 연결되는 가중치 수정\n",
    "        # 은닉계층과 출력층 사이의 가중치 행렬을 수정하는 부분입니다.\n",
    "        # 행렬 곱셈을 위해서 hidden 행렬의 모양을 바꾸기 위해서 reshape을 사용하였습니다.\n",
    "        delta_output = delta_output.reshape((1,4))\n",
    "        hidden = hidden.reshape((-1,1))\n",
    "        \n",
    "        # 가중치를 조정하기 위해서 PDF 의 공식을 구현하였습니다.\n",
    "        delta = LEARNING_RATE * np.matmul(hidden, delta_output)\n",
    "\n",
    "        # 은닉계층과 출력층 사이의 가중치 행렬을 수정해줍니다.\n",
    "        new_hidden_weight = weight_output['w'] + delta\n",
    "        weight_output['w'] = new_hidden_weight\n",
    "        \n",
    "\n",
    "# --------------------------------------------------------------------------        \n",
    "        # < Backward 부분 >\n",
    "        # hidden -> 10 x 1\n",
    "        # new hidden -> 10 x 4\n",
    "        # 입력층과 은닉계층 사이의 가중치 행렬을 수정하기 위해서 이전의 수정한 가중치 행렬을\n",
    "        # deep copy 합니다.\n",
    "        copy_hidden_weight = new_hidden_weight.copy()\n",
    "\n",
    "        # 은닉 계층에 대한 델타를 생성하기 위한 구간\n",
    "        # row =  output 계층과 연결되는 가중치 행렬 \n",
    "        # PDF의 은닉계층의 델타 공식 중 시그마 안쪽의 곱셈 부분에 해당합니다.\n",
    "        # 10 x 4\n",
    "        index = 0\n",
    "        for row in copy_hidden_weight:\n",
    "            copy_hidden_weight[index] = row * delta_output \n",
    "            index += 1\n",
    "      \n",
    "        # 1 x 10의 새로운 값 형성\n",
    "        # 시그마 부분 - 위에서 구했던 값을 이제 시그마를 통해 더해주는 구간입니다.\n",
    "        copy_hidden_weight = copy_hidden_weight.sum(axis = 1)\n",
    "      \n",
    "        # z x (1-z) 부분\n",
    "        # transpose() -> 이전의 행렬들과 곱셈을 위해서 전치 시켜줍니다.\n",
    "        # 1 x 10 행렬이 형성됩니다.\n",
    "        calculate_hidden = hidden.reshape(1,-1)\n",
    "        calculate_hidden = output_hidden(calculate_hidden)\n",
    "        \n",
    "        # z * (1-z) * delta \n",
    "        # Learning rate X z * (1-z) * delta\n",
    "        # 1 x 10 행렬이 형성됩니다.\n",
    "        new_delta = calculate_hidden * copy_hidden_weight\n",
    "        new_delta = LEARNING_RATE * new_delta\n",
    "\n",
    "        # Learning rate X delta * z(k)\n",
    "        # z(k)를 곱하기 위해서 x[n]을 전치 시켜줍니다.\n",
    "        trans_x = x[n].copy()\n",
    "        trans_x = trans_x.reshape(-1,1)\n",
    "\n",
    "        # 새로운 가중치 행렬을 위한 변화량을 최종적으로 구성합니다.\n",
    "        delta_weight =  trans_x * new_delta \n",
    "        \n",
    "        # 새로운 가중치가 형성되었습니다.\n",
    "        final_weight = weight_hidden['w'] + delta_weight\n",
    "        weight_hidden['w'] = final_weight\n",
    "    \n",
    "    result, result_hidden = forward_net(x)\n",
    "    result = output > 0.5\n",
    "    result = result.astype(int) \n",
    "    \n",
    "    accuracy = evaluation(result, y)\n",
    "    return accuracy  \n",
    "        \n",
    "# 학습을 완료한 후 테스트 데이터를 테스트하는 함수입니다.\n",
    "def run_test(test_for_x, test_for_y):\n",
    "    global test, data, weight_output, weight_hidden\n",
    "\n",
    "    output, hidden = forward_net(test_for_x)\n",
    "    output = output > 0.5\n",
    "    output = output.astype(int) \n",
    "    accuracy = evaluation(output, test_for_y)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epoch_count):\n",
    "    global data, test, weight_output, weight_hidden\n",
    "    \n",
    "    load_dataset()\n",
    "    load_testset()\n",
    "    test_x, test_y = get_test_data()\n",
    "    train_x, train_y = get_train_data()\n",
    " \n",
    "    for epoch in range(epoch_count):\n",
    "        acc = run_train(train_x, train_y)     \n",
    "        print('Epoch {}: accuracy={:5.3f}'.format(epoch+1, acc))\n",
    "  \n",
    "    last_accurancy = run_test(test_x, test_y)\n",
    "    print('\\nTest: accuracy = {:5.3f}'.format(last_accurancy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은닉 계층 하나를 갖는 다층 퍼셉트론이 작동되었습니다.\n",
      "Epoch 1: accuracy=0.750\n",
      "Epoch 2: accuracy=0.750\n",
      "Epoch 3: accuracy=0.750\n",
      "Epoch 4: accuracy=0.750\n",
      "Epoch 5: accuracy=0.750\n",
      "Epoch 6: accuracy=0.500\n",
      "Epoch 7: accuracy=0.500\n",
      "Epoch 8: accuracy=0.500\n",
      "Epoch 9: accuracy=0.500\n",
      "Epoch 10: accuracy=0.500\n",
      "Epoch 11: accuracy=0.500\n",
      "Epoch 12: accuracy=0.500\n",
      "Epoch 13: accuracy=0.500\n",
      "Epoch 14: accuracy=0.500\n",
      "Epoch 15: accuracy=0.500\n",
      "Epoch 16: accuracy=0.500\n",
      "Epoch 17: accuracy=0.500\n",
      "Epoch 18: accuracy=0.500\n",
      "Epoch 19: accuracy=0.500\n",
      "Epoch 20: accuracy=0.500\n",
      "Epoch 21: accuracy=0.500\n",
      "Epoch 22: accuracy=0.500\n",
      "Epoch 23: accuracy=0.500\n",
      "Epoch 24: accuracy=0.500\n",
      "Epoch 25: accuracy=0.500\n",
      "Epoch 26: accuracy=0.500\n",
      "Epoch 27: accuracy=0.500\n",
      "Epoch 28: accuracy=0.500\n",
      "Epoch 29: accuracy=0.500\n",
      "Epoch 30: accuracy=0.500\n",
      "Epoch 31: accuracy=0.500\n",
      "Epoch 32: accuracy=0.500\n",
      "Epoch 33: accuracy=0.500\n",
      "Epoch 34: accuracy=0.500\n",
      "Epoch 35: accuracy=0.500\n",
      "Epoch 36: accuracy=0.500\n",
      "Epoch 37: accuracy=0.500\n",
      "Epoch 38: accuracy=0.500\n",
      "Epoch 39: accuracy=0.500\n",
      "Epoch 40: accuracy=0.500\n",
      "Epoch 41: accuracy=0.500\n",
      "Epoch 42: accuracy=0.500\n",
      "Epoch 43: accuracy=0.500\n",
      "Epoch 44: accuracy=0.500\n",
      "Epoch 45: accuracy=0.500\n",
      "Epoch 46: accuracy=0.500\n",
      "Epoch 47: accuracy=0.500\n",
      "Epoch 48: accuracy=0.500\n",
      "Epoch 49: accuracy=0.500\n",
      "Epoch 50: accuracy=0.500\n",
      "Epoch 51: accuracy=0.500\n",
      "Epoch 52: accuracy=0.500\n",
      "Epoch 53: accuracy=0.500\n",
      "Epoch 54: accuracy=0.500\n",
      "Epoch 55: accuracy=0.500\n",
      "Epoch 56: accuracy=0.500\n",
      "Epoch 57: accuracy=0.500\n",
      "Epoch 58: accuracy=0.500\n",
      "Epoch 59: accuracy=0.500\n",
      "Epoch 60: accuracy=0.500\n",
      "Epoch 61: accuracy=0.500\n",
      "Epoch 62: accuracy=0.500\n",
      "Epoch 63: accuracy=0.500\n",
      "Epoch 64: accuracy=0.500\n",
      "Epoch 65: accuracy=0.500\n",
      "Epoch 66: accuracy=0.500\n",
      "Epoch 67: accuracy=0.500\n",
      "Epoch 68: accuracy=0.500\n",
      "Epoch 69: accuracy=0.500\n",
      "Epoch 70: accuracy=0.500\n",
      "Epoch 71: accuracy=0.500\n",
      "Epoch 72: accuracy=0.500\n",
      "Epoch 73: accuracy=0.500\n",
      "Epoch 74: accuracy=0.500\n",
      "Epoch 75: accuracy=0.500\n",
      "Epoch 76: accuracy=0.500\n",
      "Epoch 77: accuracy=0.500\n",
      "Epoch 78: accuracy=0.500\n",
      "Epoch 79: accuracy=0.500\n",
      "Epoch 80: accuracy=0.500\n",
      "Epoch 81: accuracy=0.500\n",
      "Epoch 82: accuracy=0.500\n",
      "Epoch 83: accuracy=0.500\n",
      "Epoch 84: accuracy=0.500\n",
      "Epoch 85: accuracy=0.500\n",
      "Epoch 86: accuracy=0.500\n",
      "Epoch 87: accuracy=0.500\n",
      "Epoch 88: accuracy=0.500\n",
      "Epoch 89: accuracy=0.500\n",
      "Epoch 90: accuracy=0.500\n",
      "Epoch 91: accuracy=0.500\n",
      "Epoch 92: accuracy=0.500\n",
      "Epoch 93: accuracy=0.500\n",
      "Epoch 94: accuracy=0.500\n",
      "Epoch 95: accuracy=0.500\n",
      "Epoch 96: accuracy=0.500\n",
      "Epoch 97: accuracy=0.500\n",
      "Epoch 98: accuracy=0.500\n",
      "Epoch 99: accuracy=0.500\n",
      "Epoch 100: accuracy=0.500\n",
      "\n",
      "Test: accuracy = 1.000\n"
     ]
    }
   ],
   "source": [
    "init_model()\n",
    "train_and_test(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
